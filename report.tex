% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{DAKI4 - MLOps Report}
\author{Jonas\\
Student number: [INDSÆT]\\
Student e-mail: [INDSÆT]@student.aau.dk\\
MLOps pipeline Github link: \url{https://github.com/katrinemie/MLOps-PJK}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
This is the hand-in template for the DAKI4 MLOps course. Please fill in your answers to the exercises and the specific documentation requirements for each course module. In the summary section, please complete the table with a summary of your answers and a reference to the full explanation. You are supposed to collaborate with others on the same project and jointly implement MLOps techniques. However, this report must be written individually.
\end{abstract}

\section*{Preface}
Reports will be subject to plagiarism checks to ensure originality. Please declare any use of Generative AI according to AAU rules, see \href{https://www.studerende.aau.dk/regler-og-praktisk/regler/regler-for-brug-af-generativ-ai}{https://www.studerende.aau.dk/regler-og-praktisk/regler/regler-for-brug-af-generativ-ai}

%%%%%%%%% BODY TEXT
\section{Introduction to MLOps}\label{sec:intro}

\subsection{D1.1: Introduction to MLOps}
Machine Learning Operations (MLOps) is a set of best practices that combines Machine Learning (ML), DevOps, and Data Engineering to manage the full lifecycle of ML models in production. While traditional software development has evolved from waterfall to agile to DevOps methodologies, ML applications present unique challenges that require a new approach.

The fundamental difference between traditional software and ML systems is that ML is not just code—it is \textbf{code plus data}. This relationship means that both code and data must evolve together in a controlled manner. Without a systematic MLOps approach, divergence between code and data evolution can cause problems in production, hinder smooth deployment, and lead to results that are hard to trace or reproduce.

\textbf{Key differences from traditional development paradigms:}
\begin{itemize}
    \item \textbf{Waterfall}: Linear, non-iterative development unsuitable for dynamic ML projects where requirements change based on model performance and data drift.
    \item \textbf{Agile}: Iterative but doesn't address the data-centric nature of ML systems or the need for continuous model monitoring.
    \item \textbf{DevOps}: Focuses on code CI/CD but lacks mechanisms for data versioning, model registry, and ML-specific monitoring.
    \item \textbf{MLOps}: Extends DevOps with three interconnected cycles—data, model, and code—ensuring reproducibility, traceability, and continuous improvement.
\end{itemize}

The MLOps workflow consists of three main phases: \textbf{Build} (data ingestion, model training, testing, packaging, registering), \textbf{Deploy} (application testing, production release via CI/CD), and \textbf{Monitor} (monitoring data integrity, model drift, and application performance). These are enabled by key drivers: data, code, artifacts, middleware, and infrastructure.

\subsection{D1.2: Project Description}
For this course, I have selected \textbf{Cats vs Dogs Image Classification} as the foundation for building an MLOps pipeline. The project involves training a deep learning model to classify images of cats and dogs using the Microsoft Cats vs Dogs dataset from Kaggle.

\textbf{Project specifications:}
\begin{itemize}
    \item \textbf{Model type}: Convolutional Neural Network (CNN)
    \item \textbf{Task}: Binary image classification (Cat vs Dog)
    \item \textbf{Dataset}: Microsoft Cats vs Dogs (Kaggle) - approximately 25,000 images (12,500 cats, 12,500 dogs)
    \item \textbf{Framework}: PyTorch
\end{itemize}

The dataset is downloaded via a custom script using the KaggleHub API, ensuring reproducible data acquisition. Data versioning is handled through DVC (Data Version Control), which is already configured in the project repository.

The project repository is available at: \url{https://github.com/katrinemie/MLOps-PJK}

\subsection{D1.3: Foreseen Challenges}
Based on the selected project, I anticipate the following challenges in developing and maintaining the MLOps pipeline:

\textbf{Development challenges:}
\begin{itemize}
    \item Ensuring code quality and adherence to PEP8 standards across the team
    \item Managing dependencies and environment consistency between development and production
    \item Implementing proper configuration management to avoid hardcoded hyperparameters
\end{itemize}

\textbf{Reproducibility challenges:}
\begin{itemize}
    \item Versioning large datasets efficiently using DVC and remote storage (MinIO/S3)
    \item Tracking experiment configurations, hyperparameters, and resulting metrics
    \item Ensuring consistent training environments across different machines
\end{itemize}

\textbf{Monitoring and maintenance challenges:}
\begin{itemize}
    \item Detecting model drift when input data distribution changes over time
    \item Setting up appropriate alerts and triggers for model retraining
    \item Balancing model performance with inference latency requirements
\end{itemize}

\subsection{D1.4: Model Card}
A model card serves as a "single source of truth" about a model, enabling full traceability from predictions back to the exact code, data, and configuration used for training. The initial draft of the model card for this project is provided in the Appendix (see \Cref{sec:appendix}).

The model card includes:
\begin{itemize}
    \item Git repository URL and commit hash
    \item Dataset name, version, and pointer to data storage
    \item Experiment tracking ID (MLflow run ID)
    \item Training environment specifications
    \item Known limitations, biases, and risks
\end{itemize}

\subsection{Implementation: Exercises}
The following exercises were completed for Module 1:

\textbf{1. GitHub Repository Setup:}
A GitHub repository was created with proper .gitignore configuration to exclude unnecessary files such as virtual environments, cached files, and sensitive credentials.

\textbf{2. Project Structure:}
The base deep-learning project was initialized with the following structure:
\begin{verbatim}
├── config/          # Configuration files
├── data/            # Data directory (ignored)
├── models/          # Model definitions
├── scripts/         # Training/testing scripts
├── requirements.txt # Dependencies
└── README.md        # Documentation
\end{verbatim}

\textbf{3. Configuration Management:}
Configuration files using YAML format were created to manage hyperparameters, ensuring no hardcoded values in the training scripts. This enables reproducible experiments and easy hyperparameter tuning.

% TODO: Tilføj screenshots af GitHub repo, config filer, etc. 

\section{Continuous ML}
Please reference all figures as Fig.~\ref{fig:figure1}, and similarly for tables.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/precision.png}
    \caption{Caption}
    \label{fig:figure1}
\end{figure}
\section{Scalable Training}
Please reference equations as Eq.~\ref{eq:equation1}.
\begin{equation}
    1+1=2
\end{equation}
\label{eq:equation1}
\section{Scalable Inference}
Please cite all sources in the following style \cite{vaswani2017attention}.

\section{Deployment}
\section{Monitoring}
\section{Guest Lecture}\label{sec:guest-lecture}
\section{Post Deployment}
\section{Summary}
\input{summary}

\section{Appendix}\label{sec:appendix}

\subsection{Model Card}
\begin{table}[h]
\centering
\caption{Model Card for Cats vs Dogs Classifier}
\label{tab:modelcard}
\begin{tabularx}{\linewidth}{@{} l X @{}}
\toprule
\textbf{Field} & \textbf{Value} \\
\midrule
\textbf{Model Name} & CatDogCNN \\
\textbf{Version} & 1.0.0 \\
\textbf{Date} & [DATO] \\
\midrule
\multicolumn{2}{l}{\textbf{Code}} \\
Repository & \url{https://github.com/katrinemie/MLOps-PJK} \\
Commit Hash & [INDSÆT COMMIT HASH] \\
Branch & main \\
\midrule
\multicolumn{2}{l}{\textbf{Data}} \\
Dataset & Microsoft Cats vs Dogs (Kaggle) \\
Version & [DVC VERSION HASH] \\
Storage & MinIO S3 bucket / Kaggle \\
Train/Val/Test Split & [f.eks. 80/10/10] \\
\midrule
\multicolumn{2}{l}{\textbf{Training}} \\
Framework & PyTorch [VERSION] \\
Environment & Docker / venv \\
Hardware & [f.eks. NVIDIA RTX Pro 4000 Ada] \\
Training Time & [TIMER] \\
MLflow Run ID & [INDSÆT] \\
\midrule
\multicolumn{2}{l}{\textbf{Hyperparameters}} \\
Learning Rate & [VÆRDI] \\
Batch Size & [VÆRDI] \\
Epochs & [VÆRDI] \\
Optimizer & Adam \\
\midrule
\multicolumn{2}{l}{\textbf{Performance}} \\
Accuracy & [VÆRDI]\% \\
Precision & [VÆRDI]\% \\
Recall & [VÆRDI]\% \\
F1-Score & [VÆRDI]\% \\
\midrule
\multicolumn{2}{l}{\textbf{Limitations \& Risks}} \\
Known Biases & Dataset contains some corrupted images that must be filtered \\
Limitations & Binary classification only (cat/dog), not other animals \\
Intended Use & Educational MLOps pipeline demonstration \\
Out-of-Scope Use & Production pet detection systems without further validation \\
\bottomrule
\end{tabularx}
\end{table}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
